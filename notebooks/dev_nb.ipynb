{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import torch\n",
    "import nltk\n",
    "import opennre\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the development notebook\n",
    "\n",
    "## Major components of the pipeline so far\n",
    "\n",
    "### 1. Create Named Entity Recognition (NER) solution\n",
    " - For this exercise the solution of [wikifier](http://wikifier.org/info.html) has been identified as a possible avenue for extracting entities of interest. The wififier is an API endpoint that takes a chunk of text and annotates entities that exists in the \"wiki\" corpus.\n",
    " - #TODO: explore additional NER solutions such as Facebook's [BLINK](https://github.com/facebookresearch/BLINK) or potentially a custom solution.\n",
    "\n",
    "\n",
    " ### 2. Load Relation Extraction (RE) Model\n",
    "- For this exercise we leverage [OpenNRE's](https://github.com/thunlp/OpenNRE) framework of pretrained relationship extraction models.\n",
    "- This pipeline initially uses the `wiki80_bert_softmax` model, which is trained on wiki80 dataset with a BERT encoder.\n",
    "- #TODO: explore additional RE models such as LUKE's `studio-ousia/luke-large-finetuned-tacred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some global variables\n",
    "\n",
    "# These are the entity types that we are after\n",
    "# They are sourced from our NER solution (in this case wikifier)\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "# load wikipedia key from .env file in root directory\n",
    "load_dotenv('../.env')\n",
    "WIKIFIER_KEY = os.environ.get(\"WIKIFIER_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikifier function\n",
    "\n",
    "def wikifier(text, lang=\"en\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Function that fetches entity linking results from wikifier.com API\n",
    "    \n",
    "    Example:\n",
    "    >>> wikifier(\"Elon Musk is the founder, CEO, CTO, and chief designer of SpaceX.\")\n",
    "    >>> [{'title': 'Elon Musk',  'wikiId': 'Q317521',  'label': 'Person',  'characters': [(0, 8), (5, 8)]}, {'title': 'SpaceX',  'wikiId': 'Q193701',  'label': 'Organization',  'characters': [(58, 63)]}\n",
    "    \"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", WIKIFIER_KEY),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # Output the annotations.\n",
    "    results = list()\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # Filter out desired entity classes\n",
    "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'wikiId': annotation['wikiDataItemId'], 'label': label,\n",
    "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of wikifier output\n",
    "test_text = \"\"\"\n",
    "Elon Musk is a business magnate, industrial designer, and engineer. \n",
    "Elon Musk is the founder, CEO, CTO, and chief designer of SpaceX. \n",
    "Elon Musk is also early investor, CEO, and product architect of Tesla, Inc. Elon Musk is also the founder of The Boring Company and the co-founder of Neuralink. A centibillionaire, Musk became the richest person in the world in January 2021, with an estimated net worth of $185 billion at the time, surpassing Jeff Bezos. Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. Elon Musk briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. Elon Musk transferred to the University of Pennsylvania two years later, where Elon Musk received dual bachelor's degrees in economics and physics. Elon Musk moved to California in 1995 to attend Stanford University, but decided instead to pursue a business career. Elon Musk went on co-founding a web software company Zip2 with Elon Musk brother Kimbal Musk.\n",
    "\"\"\"\n",
    "\n",
    "test_results = wikifier(test_text)\n",
    "pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ML models\n",
    "\n",
    "* model versions is very important here. We use `spacy==2.1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "# python -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# load openner model. This will take quite awhile to download\n",
    "relation_model = opennre.get_model('wiki80_bert_softmax')\n",
    "\n",
    "# Load NLTK\n",
    "# punkt is a tokanizer that splits text into sentances\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_threshold = 0.8\n",
    "relation_threshold = 0.5\n",
    "\n",
    "# use test text from above. This will be future source of input\n",
    "text = test_text\n",
    "entities_list = list()\n",
    "relations_list = list()\n",
    "# split text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    # for each sentence extract entities with a specified confidence\n",
    "    entities = wikifier(sentence, threshold=entity_threshold)\n",
    "    entities_list.extend(\n",
    "        [{'title': el['title'], 'wikiId': el['wikiId'], 'label': el['label']} for el in entities]\n",
    "    )\n",
    "    # permutate over entities in a sentence\n",
    "    for permutation in itertools.permutations(entities, 2):\n",
    "        for source in permutation[0]['characters']:\n",
    "                for target in permutation[1]['characters']:\n",
    "                    # for each permutation, infer a relation from our relation model\n",
    "                    data = relation_model.infer(\n",
    "                        {'text': sentence, 'h': {'pos': [source[0], source[1] + 1]}, 't': {'pos': [target[0], target[1] + 1]}})\n",
    "                    if data[1] > relation_threshold:\n",
    "                        relations_list.append(\n",
    "                            {'source': permutation[0]['title'], 'target': permutation[1]['title'], 'type': data[0]})      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see results from both the extracted entities and extracted relations. IN their intitial state there are many duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relations and entity dbs (non-deduped)\n",
    "\n",
    "relations_df = pd.DataFrame(relations_list)\n",
    "\n",
    "entities_df = pd.DataFrame(entities_list)\n",
    "\n",
    "relations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple util for deduplication\n",
    "def deduplicate_dict(d):\n",
    "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
    "\n",
    "output = {'entities': deduplicate_dict(entities_list), 'relations': deduplicate_dict(relations_list)}\n",
    "\n",
    "# final output ~ish\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27e540b88240bf3fbbaffefb089750482d49d2bf4bc5c03785510169df38391e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
