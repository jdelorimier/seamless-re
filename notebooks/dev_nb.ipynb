{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import urllib\n",
    "import torch\n",
    "import nltk\n",
    "import opennre\n",
    "import pandas as pd\n",
    "from transformers import LukeForEntityPairClassification, LukeTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some global variables\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "# wikipedia key\n",
    "WIKIFIER_KEY = \"sosnideoztzyizecctjwupsbpabuft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikifier function\n",
    "\n",
    "def wikifier(text, lang=\"en\", threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", WIKIFIER_KEY),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"nTopDfValuesToIgnore\", \"100\"), (\"nWordsToIgnoreFromList\", \"100\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"true\"), (\"ranges\", \"false\"), (\"minLinkFrequency\", \"2\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # Output the annotations.\n",
    "    results = list()\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # Filter out desired entity classes\n",
    "        if ('wikiDataClasses' in annotation) and (any([el['enLabel'] in ENTITY_TYPES for el in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([el['enLabel'] in [\"human\", \"person\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([el['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([el['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for el in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'wikiId': annotation['wikiDataItemId'], 'label': label,\n",
    "                            'characters': [(el['chFrom'], el['chTo']) for el in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test wikifier\n",
    "test_text = \"\"\"\n",
    "Elon Musk is a business magnate, industrial designer, and engineer. \n",
    "Elon Musk is the founder, CEO, CTO, and chief designer of SpaceX. \n",
    "Elon Musk is also early investor, CEO, and product architect of Tesla, Inc. Elon Musk is also the founder of The Boring Company and the co-founder of Neuralink. A centibillionaire, Musk became the richest person in the world in January 2021, with an estimated net worth of $185 billion at the time, surpassing Jeff Bezos. Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. Elon Musk briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. Elon Musk transferred to the University of Pennsylvania two years later, where Elon Musk received dual bachelor's degrees in economics and physics. Elon Musk moved to California in 1995 to attend Stanford University, but decided instead to pursue a business career. Elon Musk went on co-founding a web software company Zip2 with Elon Musk brother Kimbal Musk.\n",
    "\"\"\"\n",
    "\n",
    "test_results = wikifier(test_text)\n",
    "# pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# spacy 2.1.0 \n",
    "import spacy\n",
    "\n",
    "# make sure this is 2.1.0 or else there will be issues\n",
    "print(spacy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "# python -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# load openner model. This will take quite awhile to download\n",
    "relation_model = opennre.get_model('wiki80_bert_softmax')\n",
    "\n",
    "# Load NLTK\n",
    "# punkt is a tokanizer that splits text into sentances\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_threshold = 0.5\n",
    "\n",
    "text = test_text\n",
    "entities_list = list()\n",
    "relations_list = list()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences[:2]:\n",
    "    entities = wikifier(sentence, threshold=0.8)\n",
    "    entities_list.extend(\n",
    "        [{'title': el['title'], 'wikiId': el['wikiId'], 'label': el['label']} for el in entities]\n",
    "    )\n",
    "    for permutation in itertools.permutations(entities, 2):\n",
    "        # print(f\"permutations is {permutation}\")\n",
    "        for source in permutation[0]['characters']:\n",
    "                for target in permutation[1]['characters']:\n",
    "                    data = relation_model.infer(\n",
    "                        {'text': sentence, 'h': {'pos': [source[0], source[1] + 1]}, 't': {'pos': [target[0], target[1] + 1]}})\n",
    "                    if data[1] > relation_threshold:\n",
    "                        relations_list.append(\n",
    "                            {'source': permutation[0]['title'], 'target': permutation[1]['title'], 'type': data[0]})\n",
    "\n",
    "print(relations_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = list()\n",
    "for sentence in nltk.sent_tokenize(text):\n",
    "    sentence = strip_punctuation(sentence)\n",
    "    entities = wikifier(sentence, threshold=entities_threshold)\n",
    "    entities_list.extend(\n",
    "        [{'title': el['title'], 'wikiId': el['wikiId'], 'label': el['label']} for e in entities])\n",
    "    # Iterate over every permutation pair of entities\n",
    "    for permutation in itertools.permutations(entities, 2):\n",
    "        for source in permutation[0]['characters']:\n",
    "            for target in permutation[1]['characters']:\n",
    "                # Relationship extraction with OpenNRE\n",
    "                data = relation_model.infer(\n",
    "                    {'text': sentence, 'h': {'pos': [source[0], source[1] + 1]}, 't': {'pos': [target[0], target[1] + 1]}})\n",
    "                if data[1] > relation_threshold:\n",
    "                    relations_list.append(\n",
    "                        {'source': permutation[0]['title'], 'target': permutation[1]['title'], 'type': data[0]}\n",
    "output = {'entities': deduplicate_dict(entities_list), 'relations': deduplicate_dict(relations_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27e540b88240bf3fbbaffefb089750482d49d2bf4bc5c03785510169df38391e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
